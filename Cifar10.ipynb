{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Importing Dataset\r\n",
    "Before MLDatasets, make sure it is downloaded via Julia ], add MLDatasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "using MLDatasets\r\n",
    "\r\n",
    "# https://juliaml.github.io/MLDatasets.jl/stable/datasets/CIFAR10/#MLDatasets.CIFAR10.traintensor\r\n",
    "\r\n",
    "# load full training set as float 32, otherwise get warnings in train\r\n",
    "train_x, train_y = CIFAR10.traindata(Float32) # full traindatatset 50000 training images + 10 classes\r\n",
    "# boobs\r\n",
    "# load full test set\r\n",
    "test_x, test_y = CIFAR10.testdata(Float32) # full test dataset 10000 test images + 10 classes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import libraries "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "using Flux\r\n",
    "using Flux: Data.DataLoader\r\n",
    "using Flux: @epochs, onehotbatch, crossentropy, Momentum, update!, onecold, ADAM\r\n",
    "using Plots\r\n",
    "using CUDA\r\n",
    "using Statistics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot an Image with a Title and vlaues"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CIFAR10.trainlabels(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CIFAR10.classnames()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CIFAR10.classnames()[CIFAR10.trainlabels(1) + 1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot(CIFAR10.convert2image(CIFAR10.traintensor(1)),  title = CIFAR10.classnames()[CIFAR10.trainlabels(1) + 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Image info with Numerical Values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CIFAR10.traintensor(Float32, 1:50) # original values are gray scale 32 x 32 image size with RGB (3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train 50 Dataset (Demonstration)\r\n",
    "Conv(filter, in => out, Ïƒ = identity; stride = 1, pad = 0, dilation = 1, groups = 1, [bias, weight, init])"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First Group"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "layer1 = Flux.Conv((3,3), 3=>32, relu, pad=SamePad()) # num of paras = 3*(3*3)*32 = 864\r\n",
    "layer2 = Flux.BatchNorm(32) # this does not change when a batch is 1\r\n",
    "layer3 = Flux.Conv((3,3), 32=>32, relu, pad=SamePad())\r\n",
    "layer4 = Flux.BatchNorm(32)\r\n",
    "layer5 = Flux.MaxPool((2, 2))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second Group"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "layer6 = Flux.Conv((3,3), 32=>64, relu, pad=SamePad()) # num of paras = 3*(3*3)*32 = 864\r\n",
    "layer7 = Flux.BatchNorm(64) # this does not change when a batch is 1\r\n",
    "layer8 = Flux.Conv((3,3), 64=>64, relu, pad=SamePad())\r\n",
    "layer9 = Flux.BatchNorm(64)\r\n",
    "layer10 = Flux.MaxPool((2, 2))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Full Connection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# before this Flatten is called\r\n",
    "\r\n",
    "layer11 = Flux.Dropout(0.2)\r\n",
    "layer12 = Flux.Dense(4096, 256, relu) # (8*8)*64 features\r\n",
    "layer13 = Flux.Dense(256, 10)\r\n",
    "\r\n",
    "# layer13 goes to softmax"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dense(256, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r1 = layer1(CIFAR10.traintensor(Float32, 1:50))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r2 = layer2(r1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r3 = layer3(r2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r4 = layer4(r3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r5 = layer5(r4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r6 = layer6(r5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r7 = layer7(r6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r8 = layer8(r7)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r9 = layer9(r8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r10 = layer10(r9)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_1 = Flux.flatten(r10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_2 = layer11(full_1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_3 = layer12(full_2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = layer13(full_3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "softmax(result) # check the 6th has highest value (those values are probabilities)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating CNN: Select 1 Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Flux.Chain(\r\n",
    "    # input size = 32*32*3*1 \r\n",
    "\r\n",
    "    Flux.Conv((3,3), 3=>32, relu, pad=SamePad()),\r\n",
    "    Flux.BatchNorm(32), # should be the size of the channel dimension in your data\r\n",
    "    Flux.Conv((3,3), 32=>32, relu, pad=SamePad()),\r\n",
    "    Flux.BatchNorm(32),\r\n",
    "    Flux.MaxPool((2, 2)), # 16*16*32*1\r\n",
    "\r\n",
    "    Flux.flatten, \r\n",
    "    Flux.Dropout(0.2),\r\n",
    "\r\n",
    "    Flux.Dense(8192, 64, relu), # (16*16)*32 features\r\n",
    "    Flux.Dense(64, 10),\r\n",
    "    softmax\r\n",
    "    ) |> gpu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Full Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Flux.Chain(\r\n",
    "    # input size = 32*32*3*1 \r\n",
    "\r\n",
    "    Flux.Conv((3,3), 3=>32, relu, pad=SamePad()),\r\n",
    "    Flux.BatchNorm(32), # should be the size of the channel dimension in your data\r\n",
    "    Flux.Conv((3,3), 32=>32, relu, pad=SamePad()),\r\n",
    "    Flux.BatchNorm(32),\r\n",
    "    Flux.MaxPool((2, 2)), # 16*16*32*1\r\n",
    "\r\n",
    "    Flux.Conv((3,3), 32=>64, relu, pad=SamePad()), # 16*16*64*1\r\n",
    "    Flux.BatchNorm(64), # should be the size of the channel dimension in your data\r\n",
    "    Flux.Conv((3,3), 64=>64, relu, pad=SamePad()),\r\n",
    "    Flux.BatchNorm(64),\r\n",
    "    Flux.MaxPool((2, 2)), # 8*8*64\r\n",
    "\r\n",
    "    Flux.Conv((3,3), 64=>128, relu, pad=SamePad()), # 8*8*128*1\r\n",
    "    Flux.BatchNorm(128), # should be the size of the channel dimension in your data\r\n",
    "    Flux.Conv((3,3), 128=>128, relu, pad=SamePad()),\r\n",
    "    Flux.BatchNorm(128),\r\n",
    "    Flux.MaxPool((2, 2)), # 4*4*128*1\r\n",
    "\r\n",
    "    Flux.flatten, \r\n",
    "    Flux.Dropout(0.2),\r\n",
    "\r\n",
    "    Flux.Dense(2048, 516, relu), # (4*4)*128 features\r\n",
    "    Flux.Dense(516, 10),\r\n",
    "    softmax\r\n",
    "    ) |> gpu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "VGG16 Model : Consuming a lot of Memory (RAM)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Chain\r\n",
    "(\r\n",
    "  Conv((3, 3), 3 => 64, relu, pad=(1, 1), stride=(1, 1)), \r\n",
    "  BatchNorm(64),\r\n",
    "  Conv((3, 3), 64 => 64, relu, pad=(1, 1), stride=(1, 1)),\r\n",
    "  BatchNorm(64),\r\n",
    "  MaxPool((2,2)),\r\n",
    "\r\n",
    "  Conv((3, 3), 64 => 128, relu, pad=(1, 1), stride=(1, 1)),\r\n",
    "  BatchNorm(128),\r\n",
    "  Conv((3, 3), 128 => 128, relu, pad=(1, 1), stride=(1, 1)),\r\n",
    "  BatchNorm(128),\r\n",
    "  MaxPool((2,2)),\r\n",
    "\r\n",
    "  Conv((3, 3), 128 => 256, relu, pad=(1, 1), stride=(1, 1)),\r\n",
    "  BatchNorm(256),\r\n",
    "  Conv((3, 3), 256 => 256, relu, pad=(1, 1), stride=(1, 1)),\r\n",
    "  BatchNorm(256),\r\n",
    "  Conv((3, 3), 256 => 256, relu, pad=(1, 1), stride=(1, 1)),\r\n",
    "  BatchNorm(256),\r\n",
    "  MaxPool((2,2)),\r\n",
    "\r\n",
    "  Conv((3, 3), 256 => 512, relu, pad=(1, 1), stride=(1, 1)),\r\n",
    "  BatchNorm(512),\r\n",
    "  Conv((3, 3), 512 => 512, relu, pad=(1, 1), stride=(1, 1)),\r\n",
    "  BatchNorm(512),\r\n",
    "  Conv((3, 3), 512 => 512, relu, pad=(1, 1), stride=(1, 1)),\r\n",
    "  BatchNorm(512),\r\n",
    "  MaxPool((2,2)),\r\n",
    "\r\n",
    "  Conv((3, 3), 512 => 512, relu, pad=(1, 1), stride=(1, 1)),\r\n",
    "  BatchNorm(512),\r\n",
    "  Conv((3, 3), 512 => 512, relu, pad=(1, 1), stride=(1, 1)),\r\n",
    "  BatchNorm(512),\r\n",
    "  Conv((3, 3), 512 => 512, relu, pad=(1, 1), stride=(1, 1)),\r\n",
    "  BatchNorm(512),\r\n",
    "  MaxPool((2,2)),\r\n",
    "\r\n",
    "  x -> reshape(x, :, size(x, 4)),\r\n",
    "  Dense(512, 4096, relu),  \r\n",
    "  Dropout(0.5),\r\n",
    "\r\n",
    "  Dense(4096, 4096, relu),\r\n",
    "  Dropout(0.5),\r\n",
    "  \r\n",
    "  Dense(4096, 10),\r\n",
    "  softmax\r\n",
    "  ) |> gpu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ResNet Model: Must Run All Cells "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "struct conv_skip\r\n",
    "    conv::Chain\r\n",
    "end |> gpu\r\n",
    "conv_skip(input_filter, filter) = conv_skip(shortcut(input_filter, filter))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "function (op::conv_skip)(x, y)\r\n",
    "    z = op.conv(y)\r\n",
    "    return x + z\r\n",
    "end"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "identityBlock(input_filter, filter) = Chain(\r\n",
    "    # layer 1\r\n",
    "    Conv((3, 3), input_filter => filter, pad=(1, 1), stride=(1, 1)), # pad=(1, 1) is same\r\n",
    "    BatchNorm(filter, relu),\r\n",
    "\r\n",
    "    # layer 2\r\n",
    "    Conv((3, 3), filter => filter, pad=(1, 1), stride=(1, 1)), # pad=(1, 1) is same\r\n",
    "    BatchNorm(filter, relu),\r\n",
    "    \r\n",
    "    # layer 3\r\n",
    "    Conv((3, 3), filter => filter, pad=(1, 1), stride=(1, 1)), # pad=(1, 1) is same\r\n",
    "    BatchNorm(filter),    \r\n",
    ") |> gpu"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "identityBlock (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shortcut(input_filter, filter) = Chain(\r\n",
    "  Conv((3,3), input_filter => filter, stride=(1,1)), # pad=0 is valid\r\n",
    "  BatchNorm(filter),\r\n",
    ") |> gpu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "convBlock(input_filter, filter) = Chain(\r\n",
    "    # layer 1\r\n",
    "    Conv((3, 3), input_filter => filter, pad=(1, 1), stride=(1, 1)), # pad=(1, 1) is same\r\n",
    "    BatchNorm(filter, relu),\r\n",
    "\r\n",
    "    # layer 2\r\n",
    "    Conv((3, 3), filter => filter, pad=(1, 1), stride=(1, 1)), # pad=(1, 1) is same\r\n",
    "    BatchNorm(filter, relu),\r\n",
    "    \r\n",
    "    # layer 3\r\n",
    "    Conv((3, 3), filter => filter, stride=(1, 1)), # pad=0 is valid\r\n",
    "    BatchNorm(filter),\r\n",
    ") |> gpu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Chain(\r\n",
    "    # input size = 32*32*3*1 \r\n",
    "    \r\n",
    "    # prepare for ResNet\r\n",
    "    Conv((7,7), 3=>64, stride=(2,2)), # output = 13*13*64 : params = 3*7*7*64: \r\n",
    "    BatchNorm(64, relu),\r\n",
    "    MaxPool((3,3), pad=(1,1), stride=(2,2)), # output = 7*7*64\r\n",
    "    \r\n",
    "    # ResNet 18 layer: The simplest \"ResNet\"-type connection is just SkipConnection(layer, +)\r\n",
    "    SkipConnection(convBlock(64, 64), conv_skip(64, 64)), # output = 5*5*64\r\n",
    "\r\n",
    "    # *identityBlock must have the same input_filter and filter size. Otherwiese it gives error identityBlock(64, 128) -> error\r\n",
    "    SkipConnection(identityBlock(64, 64), +), # output = 5*5*64\r\n",
    "    SkipConnection(identityBlock(64, 64), +), # output = 5*5*64\r\n",
    "     \r\n",
    "    SkipConnection(convBlock(64, 128), conv_skip(64, 128)), # output = 3*3*128\r\n",
    "    SkipConnection(identityBlock(128, 128), +), # output = 3*3*128\r\n",
    "    SkipConnection(identityBlock(128, 128), +), # output = 3*3*128\r\n",
    "\r\n",
    "    # full connections layers\r\n",
    "    MaxPool((2, 2)), # 1*1*128\r\n",
    "    Flux.flatten, \r\n",
    "    Dropout(0.3),    \r\n",
    "    Dense(128, 1024, relu), \r\n",
    "    Dropout(0.3),\r\n",
    "    Dense(1024, 10),\r\n",
    "    \r\n",
    "    softmax\r\n",
    ") |> gpu\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Preparation ------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# without CUDA (CPU)\r\n",
    "\r\n",
    "# MLDatasets returns UInt8 thus convert it to Float64\r\n",
    "train_x = Array{Float32}(train_x) # without converting, it gives errors, especially, the labels\r\n",
    "test_x = Array{Float32}(test_x)\r\n",
    "\r\n",
    "# construct one-hot vectors from labels\r\n",
    "train_y = onehotbatch(train_y, 0:9) # https://fluxml.ai/Flux.jl/stable/data/onehot/\r\n",
    "test_y = onehotbatch(test_y, 0:9) # This is a sparse matrix, which stores just a Vector{UInt32} containing the indices of the nonzero elements.\r\n",
    "\r\n",
    "train_data = DataLoader(train_x, train_y, batchsize=50, shuffle=true) # 1000 blocks\r\n",
    "test_data = DataLoader(test_x, test_y, batchsize=10) # 100 blocks\r\n",
    "\r\n",
    "\r\n",
    "# CUDA tensor (GPU)\r\n",
    "\r\n",
    "# train_x_tensor = permutedims(train_x, [1, 2, 3, 4])\r\n",
    "# train_y_onehot = onehotbatch(train_y, 0:9)\r\n",
    "\r\n",
    "# test_x_tensor = permutedims(test_x, [1, 2, 3, 4])\r\n",
    "# test_y_onehot = onehotbatch(test_y, 0:9)\r\n",
    "\r\n",
    "# cu_train_x_tensor = cu(train_x_tensor)\r\n",
    "# cu_train_y_onehot = cu(train_y_onehot)\r\n",
    "\r\n",
    "# cu_test_x_tensor = cu(test_x_tensor)\r\n",
    "# cu_test_y_onehot = cu(test_y_onehot)\r\n",
    "\r\n",
    "# train_data = DataLoader(cu_train_x_tensor, cu_train_y_onehot, batchsize=50, shuffle=true) # 1000 blocks\r\n",
    "# test_data = DataLoader(cu_test_x_tensor, cu_test_y_onehot, batchsize=10) # 100 blocks\r\n",
    "\r\n",
    "println(\"conversion is done\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimizer ------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lr = 0.001 # learning_rate\r\n",
    "opt = ADAM(lr, (0.9, 0.999)) |> gpu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loss and Accuracy ------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# loss is for softmax\r\n",
    "loss(x, y) = sum(crossentropy(model(x), y))  # when this loss is called, assume x and y are given\r\n",
    "accuracy(x, y) = mean(onecold(model(x), 0:9) .== onecold(y, 0:9))  # when this accuracy is called, assume x and y are given\r\n",
    "\r\n",
    "# for plotting\r\n",
    "train_losses = []\r\n",
    "test_losses = []\r\n",
    "train_acces = []\r\n",
    "test_acces = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Callbacks ------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "function loss_all(data_loader)\r\n",
    "    sum([loss(x, y) for (x,y) in data_loader]) / length(data_loader) \r\n",
    "    # using sum twice: \r\n",
    "    # first sum in loss(x, y) is for batched values (each batch 50 images)\r\n",
    "    # second sum 2 lines aboive is for all batches (1000 batches)\r\n",
    "end\r\n",
    "\r\n",
    "function acc(data_loader)         \r\n",
    "    sum([accuracy(x, y) for (x,y) in data_loader]) / length(data_loader) \r\n",
    "end"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training ------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epochs = 2 # CPU takes a lot of time and memory for each epoch\r\n",
    "# epochs = 50 # GPU must be on\r\n",
    "\r\n",
    "for epoch = 1:epochs\r\n",
    "\r\n",
    "    @info \"epoch\" epoch\r\n",
    "\t\r\n",
    "\tnum = 0\r\n",
    "\t\r\n",
    "\tfor (x, y) in train_data\r\n",
    "\r\n",
    "\t\tbatch = (x, y)\r\n",
    "\t\tgs = gradient(params(model)) do\r\n",
    "\t\t\tl = loss(batch...)\r\n",
    "\t\tend\r\n",
    "\r\n",
    "\t\tnum += 1\r\n",
    "\t\tfin = (num/length(train_data))*100\r\n",
    "\r\n",
    "\t\tprint(\"...Training...\")\r\n",
    "\t\tprint(round(fin; digits=3))\r\n",
    "\t\tprint(\"%\\n\")\r\n",
    "\r\n",
    "\t\tFlux.update!(opt, params(model), gs)\r\n",
    "\t\t\r\n",
    "\tend\r\n",
    "\r\n",
    "\t@info \"...Calculating...\"\r\n",
    "\tpush!(train_losses, loss_all(train_data)),\r\n",
    "    push!(test_losses, loss_all(test_data)),\r\n",
    "    push!(train_acces, acc(train_data)),\r\n",
    "    push!(test_acces, acc(test_data))  \r\n",
    "\r\n",
    "\t@show train_loss = loss_all(train_data)\r\n",
    "\t@show test_loss = loss_all(test_data)\r\n",
    "\t@show train_acc = acc(train_data)\r\n",
    "\t@show test_acc = acc(test_data)\r\n",
    "end"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting ------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot([train_losses, test_losses], title = \"Loss\", label = [\"Training\" \"Test\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot([train_acces, test_acces], title = \"Accuracy\", label = [\"Training\" \"Test\"])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "file_extension": ".jl",
   "name": "julia",
   "mimetype": "application/julia",
   "version": "1.6.2"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.2",
   "language": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}